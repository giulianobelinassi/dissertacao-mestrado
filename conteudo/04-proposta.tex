\chapter{Parallelizing a Compiler}
\label{chap:proposta}

Having discussed both the theoretical and technical aspects of compilers, and
presenting the previous works in the area, we discuss our attempts and success
of parallelizing a compiler.

We start this chapter by presenting a profile report of GCC, which we use to
decide where to tackle (in Section \ref{sec:profile}). Then we show two ways to
proceed in order to parallelize the most costly section: by using threads (in Section
\ref{sec:threads}) or by adapting the LTO engine to compile in parallel (in
Section \ref{sec:parallel_lto}). In summary, this work will discuss the
parallelization of intraprocedural analysis inside a compiler (in this case,
GCC).

%Como atestado por \cite{PR84402}, há um gargalo de paralelismo dentro do
%próprio GCC por conta de arquivos grandes. Outro gargalo também foi relatado
%por \cite{mailgcc} em outro projeto interno. Uma das soluções para este
%problema é melhorar o paralelismo dentro do GCC, tornando possível fazer
%com que a compilação destes arquivos utilize mais núcleos de processamento.
%Nesta discussão, foi proposta uma maneira de visualizar o problema de
%paralelismo através de um gráfico gerado por dados de um GNU Make modificado.
%Como a alteração no Make é razoavelmente complicada e o \textit{script} proposto
%possuía sérios problemas de estabilidade, foi desenvolvida uma outra maneira
%de replicar os resultados.

\begin{section}{Profiling}\label{sec:profile}

We first start as an analysis of the time spent in some stages of the compiler.
As stated by \cite{PR84402}, there is a parallel compilation bottleneck in GCC
due to the existence of blob files. Therefore, the GCC itself has good samples
of files for our project. We must achieve a speedup for those to, at least, claim
that parallelizing a compiler can improve the compilation performance.
Figure \ref{fig:analysis_classical} highlights some of these files that consume
a lot of time to compile. In this figure, we collect the compilation
timestamp on the beginning and ending of each GCC file, which resulted in this
interval graph. From these files, we selected the largest one, named
\texttt{gimple-match.c}.

\begin{landscape}
%\begin{figure}[ht]
 \vspace*{-2cm}%
 \noindent%
 \hspace*{-2cm}%
    \includegraphics[width=0.9\paperheight]{seq-crop.pdf}
    \captionof{figure}{Compilation time of GCC without our modifications}
    \label{fig:analysis_classical}
%\end{figure}
\end{landscape}


\begin{figure}
\centering
	 \includegraphics[scale=1.0]{figuras/profiling-crop.pdf}
	  \caption{Profiling GCC with \texttt{gimple-match.c}}
	  \label{fig:gcc_profiling}
\end{figure}

Therefore, we used the \texttt{-ftime-report} feature from GCC and compiled
\texttt{gimple-match.c} to generate a profiling report, which we show on Figure
\ref{fig:gcc_profiling}. This profiling report was generated on a
Intel(R) Core(TM) i7-8565U, and we found that:
\begin{itemize}
    \item On average, it requires $87s$ to compile that file.

    \item 90\% of such time ($78s$) is used to optimize and generate the final code.

    \item 8\% of such time ($7s$) is used in lexical and syntatic analysis.

    \item The remaining 2\% is distributed in the remaining parts of the compiler.
\end{itemize}

These results were obtained by self-compiling GCC 10.0.1.

As GCC optimizations are divided into interprocedural and intraprocedural
optimizations, it is necessary to execute a fine-grain analysis to check which
of these consumes the most time. We found that:
\begin{itemize}
	\item 79\% of the total compilation time ($69s$) is spent in intraprocedural
	optimizations.

	\item 11\% of total compilation time ($9s$) is spent in IPA.
\end{itemize}

Therefore, a good candidate for parallelization is the intraprocedural
analysis. From the perspective of parallel computing, these optimizations can
be parallelized regarding the number of functions in the program, once they do
not require interactions with other functions.

\begin{subsection}{Theoretical Maximum Speedup}\label{sec:speedup}

On Figure \ref{fig:gcc_profiling}, the compilation of \texttt{gimple-match.c}
takes $87s$, where $79\%$ is spent on Intraprocedural Optimizations, consuming
an relevant slice of the compilation time. If we assume that we can parallelize
this part with linear speedup, and round up the time taken in the
Intraprocedural part to $80\%$ (which is $\frac{4}{5}$), then we can calculate
the parallel time $T_p$ with $p$ processors as:

$$ T_p = \frac{1}{5} T_1 + \frac{4}{5p}T_1 = \frac{1}{5} \left( 1 + \frac{4}{p}
\right)T_1 $$
where $T_1$ is the serial time we measured. By the definition of speedup,
the maximum speedup archivable in the compiler is:

\begin{equation}
\lim_{p \rightarrow +\infty} \frac{T_1}{T_p} = \lim_{p \rightarrow +\infty}
\frac{T_1}{\frac{1}{5} \left( 1 + \frac{4}{p} \right)T_1} = \lim_{p \rightarrow
+\infty} \frac{5}{1 + \frac{4}{p}} = 5
\label{eq:speedup}
\end{equation}

Therefore, if we devise an mechanism to parallelize every Intraprocedural
Optimization, we can theoretically archive up to $5\times$ speedup when compiling individual
files.

\end{subsection}

\end{section}

\begin{section}{Parallelization with Threads}\label{sec:threads}

After the IPA is complete, GCC calls the \texttt{expand\_all\_functions} function
to resume the compilation. Therefore, we designed the following mechanism: once
the IPA completes the processing of a function, it then enqueues this function
in a producer-consumer queue, which the intraprocedural passes will have to
consume from. From there, each function will be optimized in parallel, as
illustrated in Figure \ref{fig:paralelizacao}.


\begin{figure}[ht]
 \centering
 \includegraphics[width=\textwidth]{paralelizacao.pdf}
 \caption{Per-function parallelization schematic}
 \label{fig:paralelizacao}
\end{figure}

Since GCC is a huge software, we only selected a subset of the optimization
passes for this thread-based approach, which we selected the GIMPLE passes.  As
described in Section \ref{sec:opt_passes}, every pass in GCC belongs to one of
the three types: IPA, GIMPLE, or RTL, as stated in Section
\ref{sec:gcc_compiler}. Given a function, GCC runs all passes in this function
before pulling the next function in the queue. This means that if we want to
parallelize a subset of these passes (in our case, GIMPLE), we have to make
sure that the pass queue can operate in a bunch of functions simultaneously.
The first step is to make sure that we can interrupt the queue at some pass,
switch functions, execute the passes for this new function, switch back to the
original function, and everything must work.  Therefore, the first step was to
move some information available on global variables into the function struct,
so this information was not lost after this function switch.

Then we split the pass engine explicitly into three steps: IPA
transformation, GIMPLE, and RTL, and we made sure that we could run all IPA
transformations first on every function, then all GIMPLE passes in every
function, then all RTL passes in every function -- as opposed to the previous
model that applied every optimization to a single function before proceeding to
the next function. Figure \ref{fig:intraprocedural_sketch} illustrates how this
process was before our change, and \ref{fig:intraprocedural_sketch_after}
illustrates our modifications.

\begin{figure}[ht]
	\centering
	\begin{lstlisting}[
		language=pseudocode,
		style=pseudocode,
		style=wider,
		functions={},
		specialidentifiers={graph},
		]
graph* g = build_callgraph ();
ipa_perform_analysis (g);
function* cfun;
FOR_EACH_FUNCTION (g, cfun) {
  cfun->expand ();
}
	\end{lstlisting}
\caption{Sketch of GCC applying the intraprocedural optimizations}
\label{fig:intraprocedural_sketch}
\end{figure}


\begin{figure}[ht]
	\begin{center}
	\begin{lstlisting}[
		language=pseudocode,
		style=pseudocode,
		style=wider,
		functions={},
		specialidentifiers={graph},
		]
graph* g = build_callgraph ();
ipa_perform_analysis (g);
function* cfun;
working_set ws;

FOR_EACH_FUNCTION (g, cfun) {
  cfun->expand_ipa ();
}

ws.spawn_threads (expand_gimple):

FOR_EACH_FUNCTION (g, cfun) {
  ws.insert_work (cfun);
}
ws.join()

FOR_EACH_FUNCTION (g, cfun) {
  cfun->expand_rtl ();
}
	\end{lstlisting}
	\end{center}
\caption{Sketch of GCC applying the intraprocedural optimizations after our changes}
\label{fig:intraprocedural_sketch_after}
\end{figure}

After that, we chose to parallelize the GIMPLE passes. We chose GIMPLE first
because it affects every target architecture supported by GCC, whereas RTL
would require adjustment to every target architecture.  We had to insert locks
to several points in the compiler – which is the main issue with this approach
– for it to work minimally. We will discuss that below.

\begin{subsection}{Global Variables}

The first major problem was the abuse of global variables in GCC, and the lack
of replicability of critical data structures that are necessary for compiling a
function. For instance, the current function pointer (cfun) is a global
variable used in almost every intraprocedural pass in GCC, although the pass
engine itself supports passing the function that must be compiled. Other
structures required replication across the threads, for which we used the C11
Thread Local Storage feature (\textit{\_\_thread}).

Then there were the problems with global variables in most passes, which used
global variables to avoid passing them down into the call chain. Most of these
issues could be easily identified using tools like
\texttt{valgrind}\footnote{https://valgrind.org/}, once these data structures
were released after the ending of the pass, which leads to write to released memory.
This issue can be fixed by wrapping these variables inside a class and
instantiate them on the pass execution.

\end{subsection}

\begin{subsection}{Memory Pools}

Memory pools are data structures that allocate several objects of the same type
in chunks to avoid calls to the \texttt{malloc()} function, and to ensure that data is
always aligned. This functions both as optimization and to avoid memory leaks, as
one can free the entire pool at once. 

Memory pools were implemented in GCC as a class that points to one
singleton Memory Allocator object, which carries the memory allocation, and,
therefore had a serious race condition when threads tried to allocate and
deallocate memory pools. One thread could release a pool to which other threads
held pointers to, resulting in references to invalid memory; or typical race
conditions in this structure with counters, which need to be increased and
decreased as chunks of objects are allocated and released. 
As the data structure is required by other threads later in the compilation,
which is still carried by a single thread in the current state of this project,
our first attempt was to implement a Threadsafe Memory Pool allocator, which
locks a mutex each time memory is allocated or released and annotates the
thread ID on each chunk. Therefore, when memory is released, the thread only
releases the chunk they currently own. This approach made the compilation slow,
and the GCC tests failed due to time concerns. Therefore, another strategy was
designed. 

The second approach was to use a distributed memory pool. Each thread holds one
memory pool, and as a result, there is no need for locking when allocating and
releasing the chunks. This also guarantees that one thread does not release the
contents of another thread, as they have no access to pools that belongs to
other threads. However, this leads to an issue, as the data is required by
another thread later in the compilation. The solution was to implement a pool
merge feature, which merges two memory pools upon request. Since memory pools
are implemented as a linked list, the merge feature could be implemented in
$O(1)$, although the current implemented algorithm requires $O(n)$. The reason for
this is that the memory pool currently uses a single-headed linked list, and it
needs to be refactored into a double-headed linked list. 

All memory pools touched by GIMPLE intraprocedural optimizations, except for one,
were refactored with this approach. This merge feature was used only in
those memory pools which were required. The only pool which was not refactored
by using this approach was the Euler Transversal Forest data structure
(et-forest.c), simply because the compiler crashes when the strategy is
employed here. The reason for this still must be investigated.

\end{subsection}

\begin{subsection}{Garbage Collector}

GCC has an internal garbage collector, which is a reference counter of objects.
Objects that are watched by the garbage collector are declared with the GTY(())
annotation, which then an external parser calculates the size of the object.
This parser only supports a subset of C++; therefore, it requires some care
when using this resource.

Our approach is to either insert locks in
these variables or move them inside the function object. This may slowdown 
the compiler or drastically increase the memory requirement of the compiler,
as per-thread information is duplicated as per-function.

Furthermore, we fixed the issue with the Garbage Collector by ensuring that
memory allocation at this point is serialized, and disabling any memory
collection when the program is running in multi-threaded mode. This is not
necessary when multi-thread is supported by the Garbage Collector.

\end{subsection}

\begin{subsection}{Memory Address to Symbol Conversion}

GCC uses a cache to keep tracks of memory address allocated to symbols (in
\texttt{tree-sa-address.c}) and it is used to check if an address is part of a symbol
(i.e., access to an array). This is useful to determine symbol alias, for instance,
two arrays may share its memory address with another array (\textit{e.g.} pointer
copy).
This cache array is marked to be watched by the
garbage collector; therefore we lock this structure every time this array has
to be accessed.

Another way of fixing this would be clone this cache to each thread and
let it rebuild the values of already used ones. This might be faster if
locking this data structure is slower than rebuilding the object.

\end{subsection}

\begin{subsection}{Integer to Tree Node Hash}

In \texttt{tree.c}, there is a hash table used to avoid reconstruction of tree nodes
that represent integer constants. This hash is also marked to be watched by the
Garbage Collector; therefore, we used a simple lock to this structure every
time this hash was accessed. This is the same problem as the memory address to
symbol conversion, once rebuilding the structure may be faster than using the
lock.

\end{subsection}

\begin{subsection}{The rtl\_data Structure}

GCC uses a single instance of rtl\_data class, representing the current
function being compiled in RTL. This should not be a problem, as the RTL expansion
and optimization phase is still single-threaded. However, there are GIMPLE
passes that calculate instruction costs in RTL mode to decide how the function
will be optimized. This accesses the rtl\_data singleton and, therefore, exposes
a race condition that needs to be solved. To fix this issue, we have either to
replicate this structure, which is necessary to parallelize the intraprocedural
RTL optimizations, or fix the GIMPLE pass so that it does not depend on
instruction costs.

\end{subsection}

\begin{subsection}{Feasibility Discussion}

We have presented that it is possible to parallelize parts of the compiler for
parallel execution. However, using threads on an already fully developed
compiler may not be feasible from the point of development itself because of
its complexity.

The main reason is that parallelizing them incrementally is hard. The test suite
does not help on that, once they are very small programs designed to test a
certain aspect of the compiler, meaning that there is not enough stress test.
There are programs such as Helgrind and ThreadSanitizer which improve our job,
but even then, we reach a point where the race condition window is so small
that even those tools fail to detect them. There is no alternative left than
indeed understanding the code and figuring out on our own what is causing the
race condition. This certainly is the problem not only in compilers but in
large software in general.

Another issue is to guarantee that our builds are always reproducible.
Even solved race conditions may change the final executable due to order
changes in the execution that were not present in the original serial version.
This is certainly an issue if the feature is used in industrial environments.

With this in mind, perhaps the best approach is to go using a process-based
one rather than threads, which we will discuss in the next section.

\end{subsection}

\end{section}

\begin{section}{Modifying LTO for parallel file compilation}\label{sec:parallel_lto}

As presented in Section \ref{lto_section}, LTO was created to allow
cross-module optimizations in programs. It has a serial part (WPA) that imposes a
compilation bottleneck on manycore machines. On the other hand, \emph{classical
compilation} scheme can not partition its TU for parallel compilation, which
can also be a bottleneck for the compilation if it is too large.  The latter
issue, however, can be solved by transplanting the LTO partitioner to the
\emph{classical compilation} scheme and making it work \textit{without} having
the context of the \textit{entire} program. We show that this transplant is
possible by implementing it in GCC.

Here we present our approach regarding parallel compilation using process. For
that, we do simple modifications to the LTO engine.  Our approach differs from
LTO mainly in how we handle the IPA.  LTO handles these optimizations with the
context of the whole program, while our approach will only have the context of
the original TU.  This allows optimizations as good as they are in the
\emph{classical compilation} scheme while benefiting from the extra parallelism
opportunity available in the LTO's LTRANS stage.

In this section, we will discuss some of our design decisions based on the
internal workings of GCC. In Subsection \ref{sec:gcc_driver}, we present how we
modified the GCC driver to reconstruct the output object file, required for
maintaining compatibility building tools such as Make. Then, in
Subsection \ref{sec:lto_partitioner}, we present a short algorithm for making
the LTO partitioner work for our proposal. In Subsection
\ref{sec:partition_mask}, we present a necessary change we had to do in our work
about how partitions are applied in GCC. In
Subsection \ref{sec:name_clash_resolution}, we explain how we solved the issue
of private symbols with the same name being promoted to global.
Then in Subsection \ref{sec:integration_jobserver}, we present an optional part
of our work about communicating with the GNU Make Jobserver to keep track of
used processors. Finally, we discuss why our proposal is still compatible
with the Reproducible Builds project in Subsection \ref{sec:repro_builds}.

\begin{subsection}{The \texttt{gcc} Driver}\label{sec:gcc_driver}

A large program can be written in several languages, with each of them having
its own compiler. As stated in Section \ref{sec:motivation}, a compiler
translates a program from a language $A$ into another language $B$.
In GCC, it translates several languages to assembly
of some architecture (\textit{e.g.} x86). This means that
encapsulating code in object files, or linking these files in an executable, are
not tasks of the compiler. However, the user can launch \texttt{gcc -o binary
file.c} and get a working binary. That is because the binary \texttt{gcc} is
a \textit{driver}, and it will launch the
necessary programs for the user. In fact, this line launches three programs,
as illustrated in Fig. \ref{fig:gnu_toolchain}.

Therefore, if we want our changes to not to break the building scripts
(\textit{e.g.}, Makefile) used by most projects -- which is mostly launching
\texttt{gcc file.c -c} that creates an object file \texttt{file.o} -- we must
ensure that we create a single object file for each file, not multiple ones, as does
the LTO partitioner. Fortunately, we can rely on GNU \textit{ld} partial linking
for merging object files into one. Therefore, the solution to this problem is:
\begin{enumerate}
	\item Patch the LTO \textit{partitioner} to communicate the location of
	each created assembly file (.s) to the \textit{driver}. This can be
	done by passing a hidden flag \texttt{-fadditional-asm=<file>}
	by the driver to the partitioner, in which the last will write to. This file can also
	be replaced with a Named Pipe for better performance if needed.

	Then, the partitioner checks if this flag has been passed to the compiler.
	If so, then a \textit{compatible version} of the driver is installed. If
	the partitioner decides to partition the TU, it should \textit{retarget}
	the destination assembly file and write the retargeted name to the
	communication file.

	\item Patch the driver to pass this hidden flag to the
	\textit{partitioner}, and also to check if this file exists. If not, this means
	that either the compiler is incompatible or it has chosen not to partition
	the TU. In the first case, the driver should call the assembler to every
	assembly file generated, and call the linker to generate the expected
	final object file. In the second case, simply fallback to the previous
	compilation logic.
\end{enumerate}

Fig. \ref{fig:gnu_toolchain_patched} illustrates the code flow after these
changes.  The execution starts in the highlighted node \textit{gcc}, which
calls the compiler (cc1) with the necessary flags to establish a communication
channel among the parts. The compiler then will partition the TU and forks
itself into several child processes, one for each partition.

Once multiple processes are created, the compiler will communicate its output
.s file, and the driver then will launch the assembler (\textit{as}) to
generate several object files, and then launch the linker (\textit{ld}) to
merge them all into a single object file.

\begin{figure}
\tikzstyle{block} = [rectangle, draw, fill=white,
    text width=6em, text centered, rounded corners, minimum height=2em]
\tikzstyle{line} = [draw, -latex]

\makebox[\textwidth][c]{
\scalebox{0.8}{
\begin{tikzpicture}[node distance = 3cm, auto]
    % Place nodes
    \node [block]                      (cc1_1) {cc1};
    \coordinate [right= of cc1]          (c);
    \node [block, right= of c,fill={rgb:black,1;white,2}]        (driver1) {gcc};
    \node [block, above= of c]                      (cc1_2) {cc1};
    \node [block, below= of c]                      (cc1_3) {cc1};
    \coordinate [above= of driver1]          (c2);
    \coordinate [below= of driver1]          (c3);
    \node [block, right= of c2]      (as2) {as};
    \node [block, right= of c3]      (as3) {as};
    \node [block, right= of driver1]       (ld) {ld};
    \coordinate [left= of cc1]          (fonte);
    \coordinate [right= of ld]    (bin);

    % Draw edges
    \draw[->]    (driver1.west)    -- (cc1_1.east) node[midway, above] {\texttt{-fadditional-asm=<file>}};
    \draw[->]    ([xshift=+0.5cm]cc1_2.south) -- ([xshift=-0.5cm]driver1.north) node[midway, above, sloped] {Generates .s file};
    \draw[->]    ([xshift=+0.5cm]cc1_3.north) -- ([xshift=-0.5cm]driver1.south) node[midway, above, sloped] {Generates .s file};

    \draw[->]    (cc1_1.north)    -- ([xshift=-0.5cm]cc1_2.south) node[midway, above, sloped] {Forks};
    \draw[->]    (cc1_1.south)    -- ([xshift=-0.5cm]cc1_3.north) node[midway, above, sloped] {Forks};

    \draw[->]    ([xshift=+0.5cm]driver1.north)  -- (as2.south) node[midway, above, sloped] {Assemble received .s};
    \draw[->]    ([xshift=+0.5cm]driver1.south)  -- (as3.north) node[midway, above, sloped] {Assemble receibed .s};
    \draw[->]    (driver1.east)  -- (ld.west) node[midway, above, sloped] {Links};

%    \draw[->]    (cc1.east)    -- (as.west)       node[midway, above] {Assembler File};
%    \draw[->]    (cc1.east)    -- (as.west)       node[midway, above] {Assembler File};
%
%    \draw[->]    (cc1.east)    -- (as.west)       node[midway, below] {(.s)};
    \draw[->]    ([xshift=+0.5cm]as2.south)     -- (ld.north)       node[midway, above, sloped] {Object File};
    \draw[->]    ([xshift=+0.5cm]as3.north)     -- (ld.south)       node[midway, below, sloped] {Object File};
%    \draw[->]    (fonte.west)  -- (cc1.west)      node[pos=0, above] {Source File};
%    \draw[->]    (fonte.west)  -- (cc1.west)      node[pos=0, below] {(.c)};
    \draw[->]    (ld.east)     -- (bin.west)      node[pos=1, above] {\texttt{file.o}};
\end{tikzpicture}
}
}%
\caption{Interaction between the \textit{driver}, the \textit{compiler}, and other tools
after our changes}
\label{fig:gnu_toolchain_patched}
\end{figure}

\end{subsection}

\begin{subsection}{Adapting the LTO partitioner}\label{sec:lto_partitioner}

In GCC, a TU is represented as a callgraph. Every function, global variable, or
clones (which may represent a function to be inlined) is represented as nodes
in a callgraph. If there is a function call from $f$ to $g$, or a
reference to a global variable $g$ in $f$, then there is an edge from $f$ to
$g$. This means that TU partitioning can be represented as a \emph{graph
partitioning} problem.

When LTO is enabled and GCC is in the WPA stage, the body of these nodes
is not present, just a \textit{summary} of them (\textit{e.g.} the size
in lines of code it had). This is done to save memory. However,
when LTO is disabled, the function body is present. Therefore,
the LTO WPA engine must be further patched to expect that the function
body is present when analyzing the program. 

Then comes the partitioner algorithm \textit{de facto}. In LTO, GCC tries to
create partitions of similar size, and always try to keep adjacent nodes together.
However, we devised an partitioning algorithm for our tests which only keeps
together nodes which are \textit{required} to be in the same partition. Therefore,
we do not focus on partition balance, although we did implement a very simple
partition merging logic for some balance.

This partitioning algorithm works as follows: for each node, we check if it is
a member of a COMDAT \citep{comdat} group, and if merged it into the same
partition. We then propagate outside of this COMDAT group; checking for every
node that may trigger the COMDAT to be copied into other partitions and group
them into the same partition. In practice, this means to include every node hit
by a Depth-First Search (DFS) from the group to a non-cloned node outside of
the group.  Figure \ref{alg:comdat} presents a pseudocode of this process.
Most of this process is already present from the COMDAT dependency algorithm
from LTO.  We added the logic to expand the partition frontier if a duplicated
symbol is encountered, adding further non-duplicated symbols into
the boundary, as we illustrate in Figure \ref{fig:comdat_frontier}. Notice how
$v_{16}$ is not added to the frontier, as it is called by $v_{14}$, the symbol marked
as duplicated.

\begin{figure}
\centering
	 \includegraphics[scale=0.7]{figuras/comdat_frontier.pdf}
	  \caption{Example of callgraph, in beige being represented by the COMDAT group,
	  in green by the COMDAT frontier, and in red, by the cloned nodes.}
	  \label{fig:comdat_frontier}
\end{figure}


\begin{figure}[ht]
	\begin{center}
	\begin{lstlisting}[
		language=pseudocode,
		style=pseudocode,
		style=wider,
		functions={},
		specialidentifiers={graph},
		]
\textbf{Partitions} ds // Hold partitions. Can be implemented with Union-Find.

procedure lto_merge_comdat_map(Callgraph $G$) // To be called by the LTO partitioner
	\textnormal{Initialize} ds // Initialize one partition per node. 
	for \textnormal{each symbol of} $G$ \textbf{as} $v$
		if $v$ \textnormal{belongs to a COMDAT group}
			\texttt{merge_comdat_nodes}($v$, \textnormal{partition of} $v$)
		\texttt{merge_contained_symbols}($v$, \textnormal{partition of} $v$)
	\textnormal{balance the partitions in} ds
	\textnormal{builds LTRANS partitions object} \textbf{from} ds

procedure merge_comdat_nodes(node $v$, Partition $set$) // Merges COMDAT group with its frontier
	if $v$ \textnormal{is marked} return
	\textnormal{mark} $v$
	if $v$ \textnormal{belongs to a COMDAT group}
		for \textnormal{every node in the same COMDAT group of} $v$ \textbf{as} $w$
			\textnormal{unite partition of} $w$ \textnormal{with partition} set
			\texttt{merge_comdat_nodes}($w$, $set$)
	if $v$ \textnormal{belongs to a COMDAT group} \textbf{or} $v$ \textnormal{is a duplicated symbol}
// If $v$ is duplicated and not member of COMDAT group, we need to reach to a non-duplicated symbol to add to the frontier.
		if $v$ \textnormal{is a function}
			for \textnormal{every caller of} $v$ \textbf{as} $u$
				if \textnormal{calledge} $(u, v)$ \textnormal{is inlined} \textbf{or} $v$ \textnormal{is a duplicated symbol} // Same as above
					\textnormal{unite partition of} $u$ \textnormal{with partition} set
					\texttt{merge_comdat_nodes}($u$, $set$)
// thunk functions adjust their object \texttt{this} pointer before calling the function
			if $v$ \textnormal{is a thunk function} \textbf{and} $v$ \textnormal{is not inlined anywhere}
				for \textnormal{every callee of} $v$ \textbf{as} $w$
					\textnormal{unite partition of} $u$ \textnormal{with partition} set
					\texttt{merge_comdat_nodes}($u$, $set$)
// Look at all nodes refering $v$, and add them to the COMDAT partition as well
		for \textnormal{every node referring} $v$ \textbf{as} $u$
			\textnormal{unite partition of} $u$ \textnormal{with partition} $set$
			\texttt{merge_comdat_nodes}($u$, $set$)
			$u$ \textnormal{is a duplicated symbol} // we must reach a non-duplicated symbol for the frontier
				\texttt{merge_comdat_nodes}($u$, $set$)

procedure merge_contained_symbols(node $v$, Partition $set$)
	for \textnormal{every node contained in} $v$ \textbf{as} $u$
		\textnormal{unite partition of} $u$ \textnormal{with partition} set
	\end{lstlisting}
	\end{center}
\caption{Partition the callgraph into multiple partitions for parallel compilation.}
\label{alg:comdat}
\end{figure}

At first, we also did this process for private functions to avoid
promoting them to public, once external access would be necessary if they go
into distinct partitions. However, results showed that this has a strong
negative hit in any parallelism opportunity.

For grouping the nodes together,
we used a Union Find with Path Compression, which yields an attractive
computational complexity of $O(E + N \lg^*N)$ to our partitioner, where $N$ is the
number of nodes and $E$ is the number of edges in the callgraph \citep{feufiloff}.

Once the partitions are computed, we need to compute its \textit{boundary}.
If function $f$ calls $g$, but they were assigned into distinct partitions,
then we must include a version of $g$ in $f$'s partitions without its body,
then check if $g$ is a private function. If it is, then $g$ must be promoted
to a public function. There is also extra complexity if a version of $g$
is marked to be inlined in $f$, which means that its body has to be
streamed somehow. Fortunately, most of this logic is already present
in LTO and we could reuse them. However, some issues were found
when handling inline functions and global variables marked as part
of the boundary. First, some functions marked to be inlined into 
functions inside the partition were incorrectly marked to be removed.
We fixed that by checking if a function planned to be removed is inlined
into a function inside the boundary; secondly, being variables marked as in the boundary (and therefore,
not in the partition) not being correctly promoted to external.
We further fixed that by checking if there were references to that variable
through other partition, and in this case we promoted them to global.

Furthermore, there were some issues concerning how GCC handles
partitions, which we will discuss in the next subsection.

\end{subsection}

\begin{subsection}{Applying a Partition Mask}\label{sec:partition_mask}

Once partitions are computed, the only presented way to apply it
(\textit{i.e.}, remove every unnecessary node from the TU) was to reload the
compiler and let it load the phony object files. The issue is that these
objects are not available in our project because we are not running in LTO
mode. We developed another method for this.
We used the Unix \textit{fork} function to spawn a child process, and then
we implemented our own method to apply the partition without having to load
these phony object files. Fortunately, this consisted
of four cases:
\begin{itemize}
	\item \textit{Node is in partition}: nothing has to be done;
	\item \textit{Node is in boundary, but keeping its body}: we mark that this function
	is available in other partitions, but we do not release its body or
	data structures;
	\item \textit{Node is in boundary}: we mark this mode as having its body removed,
	but we never actually remove it. This is because the node may share the
	body contents with another node in the partition. We then remove
	every edge from its functions to its callees, all the references to variables,
	the content of the Dominator Tree, and also its Control-Flow Graph. This
	is now a function which this partition only knows that it exists.
	\item \textit{Node not in boundary}: we remove the node and all its content.
\end{itemize}

After this, it creates a new output assembly file private to
this partition, and writes a pair (\textit{partition number, path to file}) into the
communication file, which the driver will read in the future. The partition
number is important to guarantee that the build is reproducible, as we will
discuss later.

It is also important that some early steps in the compiler do not emit
assembly too early, such as was the case of the gimplifier in GCC, or else the
output file will be incomplete. These cases could be easily detected by using
a breakpoint and flushing the assembler output file before retargeting the new
assembler output file.

Once the partition is applied to the child process, and the output file has
been communicated to the driver, it can continue with the compilation. It
should be running in parallel now by the number of partitions.

\end{subsection}

\begin{subsection}{Name Clash Resolution}\label{sec:name_clash_resolution}

In LTO, if there are two promoted symbols to global with the same (mangled) name, a
straightforward way to fix that is to increment a counter for each clashed
symbol. This is possible because LTO has the context of the entire program,
which we do not. Therefore, we need another way of fixing it.

Two naive approaches would be to select a random integer and append it to the
function's name, or append the memory address of the function object to the
function's name.  Both of them break bootstrap \citep{bootstrap}, the first
one does because output functions will have distinct names on every new compilation; The
second, because stage 1 compiles with \texttt{-O0} and stage 2 with
\texttt{-O2}, which changes the memory address of the functions between these stages.

Our solution is to use a crc32 hash of the original file and append it to the
function's name.  There is still a tiny probability of name clash with this
approach; however, we did not find any on our tests.

\end{subsection}

\begin{subsection}{Integration with GNU Make Jobserver}\label{sec:integration_jobserver}

GNU Make can launch jobs in parallel by using \texttt{make -j}.  To
avoid unnecessary partitioning and job creation when the CPU is at 100\% usage,
we have also implemented an integration mechanism with GNU Jobserver
\citep{posixjobserver}.  The implementation is simple: we query the server for
an extra token. If we receive the token, then it means that some processor is
available, and we can partition the TU and launch jobs inside the compiler.
Otherwise, the processor workload is full, and it may be better to avoid
partitioning altogether.

However, for a program to be able to communicate with the jobserver, it should
be launched with a prepended \texttt{+} character (\textit{e.g.} $\texttt{+gcc
-c file.c}$); therefore it is not so straightforward to use this mode on
existing projects.

\end{subsection}

\begin{subsection}{Relationship with Reproducible Builds}\label{sec:repro_builds}

One interesting point of Open Source is that it can be verified by everyone.
However, very often these projects are distributed in a binary form to the
users, removing from them the burden of this process. Nevertheless, nothing avoids that a
malicious developer modifies the codebase \textit{before} the distribution
(\textit{e.g.} inserting a backdoor), and claiming that he/she got a distinct
binary because his/her system is different from the user.

The Reproducible Builds project aims to solve that issue by providing a way to
reproduce the released binary from its source. Some software needs to
be patched to work with Reproducible Builds, for instance,
to not contain some kind of build timestamp, and so on. A build
is called \textit{reproducible} if, given the same source code and build
instructions, anyone can recreate a bit-perfect version of the distributed
binary \citep{reproducible_builds}.

To keep compatibility with this project, we must ensure that our compiler
always outputs the same code with a given input. The input is not only
the source file itself but also the flags passed to it.

We claim that our modification still supports the Reproducible Builds because
of the following reasons:

\begin{itemize}
	\item No random information is necessary to solve name clashing;
	\item Given a number of jobs, our partitioner will always generate
	the same number of partitions for a certain program, always with the same content;
	\item Partial Linking is always done in the same order. To ensure that,
	we communicate to the driver a pair (\textit{partition number, path to file}),
	and we sort this list using the partition number as the key;
	\item No other race conditions are introduced, as we rely on the quality of
	the LTO implementation of the compiler;
\end{itemize}

However, there is one point of concern, which is the Jobserver Integration.  If
the processor is already in 100\% usage, we avoid partitioning at all and
proceed with sequential compilation. This certainly changes accordingly to the
processor usage of the machine during the build; therefore, the build is not
guaranteed to be reproducible if this option is enabled. This is not an issue
if the number of jobs is determined beforehand.

\end{subsection}

\begin{subsection}{Feasibility Discussion}

We have presented a way to adapt the LTO engine to compile single files in
parallel. When compared to the threaded version, this work resulted in greater
speedups (2.4x vs. 1.61x, as presented in Sections \ref{sec:threaded-results}
and \ref{sec:lto-results}) and less time required in development. This is
because a lot of the LTO logic could be reused and the problem changed
drastically from ensuring that there are no race conditions to finding the
correct resources to share. Incremental development is also easier here, once
we knew that the sources of issues would be the partitioner or the partitioner
applier. Not only that, given an input file, the issues are reproducible,
removing the painful randomness of race conditions from the problem.

The good news is that LTO is present in almost every industrial-scale compiler
nowadays. Therefore, implementing single-file parallelism on those by using our
methods would be significantly less cumbersome than making sure that the
threaded implementation is correct.

Therefore, if the compiler already has LTO support and the developers wants
to improve the performance of the compiler in manycore environments, this is a
quick, large outcome method of doing that.


\end{subsection}
\end{section}

%Neste capítulo é apresentado um plano de trabalho que será executado
%após o período de qualificação, bem como resultados esperados.
%Este trabalho tem o objetivo de trazer duas contribuições:
%\begin{enumerate}
%    \item Uma revisão sobre o que pode ser feito para acelerar o
%processo clássico de compilação em máquinas \textit{manycore} através
%de paralelismo, e quais problemas devem ser atacados em trabalhos futuros.
%
%    \item Uma opção no compilador GCC para que ele seja
%capaz de compilar um único arquivo em paralelo sem utilizar a
%estrutura do LTO. Isso é útil para desenvolvedores que trabalham
%com grandes softwares iterativamente, pois deve minimizar
%gargalos gerados por arquivos grandes e cobrir os casos onde o LTO gera
%binários menos eficientes, fornecendo assim uma alternativa a essa tecnologia.
%Este trabalho se concentra em paralelizar os passos de otimização Intra Procedural
%a nível de funções, ou seja, duas análises executarão em paralelo em funções
%distintas, evitando adentrar em paralelizar a análise de fluxo de dados. Espera-se
%utilizar as informações do item 1 para realizar um estudo de caso.
%\end{enumerate}
%O segundo item será enviado ao GCC como uma contribuição ao \textit{software}
%livre. O projeto de paralelização foi submetido e aprovado para o GSoC 2019,
%conforme o Apêndice \ref{ap:gsoc}.


%\section{Paralelização do GCC com \textit{threads}}
%
%Nesta subseção é apresentado o estado atual do projeto de paralelização
%do GCC com \textit{threads}. O GCC foi escolhido como candidato para
%paralelização pois a comunidade demonstrou grande interesse no projeto,
%conforme discutido na Seção \ref{cap:introducao}, e também devido a familiaridade
%do autor com o projeto, já tendo previamente contribuído com este. Sendo assim,
%utilizar outro compilador como o Clang para implementar o projeto demandaria
%estudos sobre a estrutura do projeto e convencimento da comunidade das possíveis
%vantagens deste trabalho ao projeto. Por outro lado, implementar um
%novo compilador não é uma alternativa viável dado que um dos maiores gargalos
%na compilação é o otimizador, peça que não seria possível implementar em
%dois anos com o mesmo poder das já existentes no GCC ou no Clang.
%
%Como atestado por \cite{PR84402}, há um gargalo de paralelismo dentro do
%próprio GCC por conta de arquivos grandes. Outro gargalo também foi relatado
%por \cite{mailgcc} em outro projeto interno. Uma das soluções para este
%problema é melhorar o paralelismo dentro do GCC, tornando possível fazer
%com que a compilação destes arquivos utilize mais núcleos de processamento.
%Nesta discussão, foi proposta uma maneira de visualizar o problema de
%paralelismo através de um gráfico gerado por dados de um GNU Make modificado.
%Como a alteração no Make é razoavelmente complicada e o \textit{script} proposto
%possuía sérios problemas de estabilidade, foi desenvolvida uma outra maneira
%de replicar os resultados.
%
%Como desenvolvido e publicado por \cite{gcctimer}, a ferramenta aqui proposta
%é capaz de coletar e exibir dados referente ao tempo de compilação
%de cada arquivo no GCC, incluindo os testes gerados pelo GNU Autotools.
%A ferramenta funciona da seguinte forma: há um programa escrito em C chamado
%\texttt{cc\_wrapper} que encapsula o compilador C e C++ do ambiente, no caso o 
%GCC e o G++. O caminho para estes compiladores são passados como um parâmetro
%da compilação do \texttt{cc\_wrapper} de maneira que os binários gerados os
%simulem. Em seguida o programa abre um novo processo através do \texttt{fork()},
%chamando o GCC/G++ com os parâmetros passados a ele sem alterações. O processo
%inicia a coleta do tempo, busca pelo nome do arquivo objeto a ser gerado, e
%aguarda o GCC chamado terminar. Essa busca foi codificada de maneira a ser
%muito eficiente, tento um pior caso $O(n)$ com uma constante muito baixa,
%onde $n$ é o número de parâmetros passados ao GCC. Em seguida, o programa
%escreve o tempo de início, tempo de fim, e o nome do arquivo em um arquivo
%de texto. Houve cautela para que não haja mistura de linhas
%no arquivo por razão de escrita simultânea no arquivo. Há também um \textit{script}
%em \textit{Bash} para reproduzir os resultados facilmente.
%
%Também é disponibilizado pelo autor um programa em \textit{Python} para análise dos
%resultados, responsável por gerar os gráficos conforme
%mostrado na Figura \ref{fig:analysis_classical}. Em um dos eixos há o
%tempo de execução, no outro há
%o trabalho do Makefile. Para construir o gráfico, é utilizado a técnica
%de coloração de grafos de intervalos: cada nó representa um intervalo de
%tempo, e é adicionado uma aresta entre esses nós caso haja intersecção nos
%intervalos. Em seguida, é executado um algoritmo guloso de coloração a
%partir do tempo de inicio. Como o grafo é de intervalos, esse algoritmo
%garante que a quantidade de cores utilizadas é a menor possível. Isso dá
%uma boa estimativa de como os trabalhos foram distribuidos pelo Makefile.
%Do ponto de vista de complexidade, isso garante que os gráficos podem ser
%gerados em $O(n \log p)$, onde $n$ é o número de arquivos e $p$ é o número de
%cores, embora o algoritmo implementado seja $O(np)$.
%
%\subsection{Investigação do Tempo Consumido na Compilação}
%
%Uma investigação foi conduzida com a finalidade de encontrar o gargalo
%principal no processo de compilação do GCC. Todos os testes efetuados foram
%executados em um computador com um AMD Opteron 6376 (64 núcleos) executando
%o Debian 10 (\textit{Testing}).
%
%Primeiro, foi executado um experimento com respeito ao tempo de compilação
%do GCC com o LTO habilitado, e outro desabilitado. Em ambos os experimentos,
%o processo de \textit{bootstrap} do compilador foi desabilitado. Para o caso LTO,
%utilizamos no máximo 64 \textit{threads} através da diretiva de compilação
%\texttt{-flto=64} para permitir o uso de paralelismo nesse método. Como
%é possível concluir comparando as Figuras \ref{fig:analysis_lto} e \ref{fig:analysis_classical},
%o LTO consome cerca de 1 minuto a mais para compilar todo o programa
%(mais de 200s do LTO, contra pouco mais de 140s sem o LTO). Sendo assim,
%desabilitar o LTO mesmo que ele forneça um grau maior de paralelismo é
%mais vantajoso caso o objetivo seja compilar mais rapidamente.
%
%Analisando o paralelismo do processo clássico na Figura \ref{fig:analysis_classical},
%é possível notar dois itens, independente da quantidade de
%execuções do experimento:
%
%\begin{itemize}
%    \item A existência de arquivos como o \texttt{gimple-match.c}, que
%        geram um gargalo no paralelismo do GCC.
%
%    \item Várias etapas sequênciais executadas pelo GNU Autoconf.
%\end{itemize}
%
%O arquivo \texttt{gimple-match.c} é gerado automaticamente compilando
%o arquivo \texttt{match.pd} para C. Na versão 9.0.1 do GCC, o arquivo
%gerado contém exatamente 99329 linhas de código. Espera-se que o tempo
%de compilação do \texttt{gimple-match.c} diminua, em conjunto com o tempo
%total de compilação, ao paralelizar o GCC com \textit{threads}.
%
%Analisando o tempo necessário para compilar o arquivo \texttt{gimple-match.c},
%foi possível notar que:
%\begin{itemize}
%    \item São necessários em média 76 segundos para compilar tal arquivo.
%
%    \item 91\% desse tempo (69 segundos) é utilizado na etapa de otimização
%        e geração de código final.
%
%    \item 8\% desse tempo (6 segundos) é utilizado na etapa de análise léxica
%        e sintática.
%
%    \item O outro 1\% está distribuído em diversas partes do compilador.
%\end{itemize}
%Todos estes dados foram obtidos autocompilando o GCC 9.0.1, e utilizando a
%ferramenta de \textit{profiling} incorporada
%no GCC através das flags \texttt{-ftime-report} \texttt{-ftime-report-details}.
%
%Como as otimizações do GCC são divididas em IPA, GIMPLE e RTL, é necessário
%executar uma granularidade mais fina na análise. Com uma simples alteração
%no GCC, foi possível separar a etapa IPA das demais. Bastou embrulhar as funções
%\texttt{ipa\_passes()} e \texttt{expand\_all\_functions()} com duas \textit{timevars}
%distintas. Assim, os seguintes dados foram obtidos:
%\begin{itemize}
%    \item 75\% do tempo total de compilação (57s) é gasto nos passos de otimização
%        Intra Procedural e geração de código.
%
%    \item 11\% do tempo total de compilação (11s) é gasto para realizar as IPA.
%\end{itemize}
%Sendo assim, o principal candidato a paralelização é a função \texttt{expand\_all\_functions()},
%responsável pelas otimizações Intra Procedural e geração de código.
%Entretanto, para realizar tal paralelização, será necessário documentar e remover diversas
%variáveis globais do GCC de maneira que seja possível realizar paralelismo com \textit{threads}.
%
%\subsection{Análise do \textit{Speedup} Máximo}
%
%Conforme analisado acima, se paralelizarmos as etapas de otimização Intra
%Procedural e Geração de Código, é possível paralelizar 75\% do tempo total.
%Portanto, seja $p$ o número de processadores utilizados. Assumindo
%\textit{speedup} linear, o ganho máximo será:
%
%$$ T_p = \frac{1}{4} T_1 + \frac{3}{4p}T_1 = \frac{1}{4} \left( 1 + \frac{3}{p}
%\right)T_1 $$ Sendo assim, o \textit{speedup} máximo por arquivo será: $$
%\lim_{p \rightarrow +\infty} \frac{T_1}{T_p} = \lim_{p \rightarrow +\infty}
%\frac{T_1}{\frac{1}{4} \left( 1 + \frac{3}{p} \right)T_1} = \lim_{p \rightarrow
%+\infty} \frac{4}{1 + \frac{3}{p}} = 4$$
%Entretanto, é muito provável que o \textit{speedup} a ser obtido seja menor que isto
%devido a mecanismos de sincronização necessários para implementar o paralelismo.
%
%\subsection{Arquitetura da Paralelização}
%
%A arquitetura da paralelização que será implementada no GCC é similar ao
%descrito por \cite{wortman1992}: serão executadas um número fixo de
%\textit{threads} trabalhadoras de acordo com o número de processadores
%disponíveis no computador.
%Assim que o IPA finalizar o plano de
%de otimização, as funções serão alimentadas em uma fila Produtor-Consumidor
%onde as \textit{threads} trabalhadoras retirarão o trabalho.
%Cada \textit{thread} será responsável por executar todas as passagens
%de otimização do GIMPLE em uma função retirada da fila produtor-consumidor.
%Sendo assim, duas \textit{threads} processarão duas funções distintas.
%Esse processo está retratado na Figura \ref{fig:paralelizacao}.
%Em seguida, será avaliado o \textit{speedup} ao paralelizar o GIMPLE, e
%será decidido qual outro problema abordar: paralelização das demais
%passagens RTL, ou uma tentativa de paralelizar o IPA.
%
%\begin{figure}[ht]
% \centering
% \includegraphics[width=\textwidth]{paralelizacao.pdf}
% \caption{Esquema de paralelização dos passos de otimização.}
% \label{fig:paralelizacao}
%\end{figure}
%\subsection{Experimentos e Metodologia}
%
%Para validar os resultados obtidos, serão executados dois experimentos:
%\begin{enumerate}
%    \item Comparação do tempo de compilação do arquivo \texttt{gimple-match.c}
%        antes e após a paralelização.
%
%
%    \item Comparação do tempo total de compilação do GCC antes e após a paralelização.
%\end{enumerate}
%Tal comparação será realizada utilizando
%técnicas de inferência estatística, conforme descrito por
%\cite{morettin2017estatistica}: serão
%coletadas diversas amostras a respeito do tempo de compilação. Em seguida, será feita alguma suposição a respeito da distribuição
%das amostras de acordo com o resultado de um gráfico Q-Q para então calcular um estimador
%para o valor esperado e variância. Com isso, serão calculados os intervalos de confiança
%sobre a média do tempo de execução para afirmar que ela diminui após a paralelização. Caso
%não seja possível encontrar uma distribuição adequada, será utilizado o Teorema do Limite
%Central com um número grande de amostras. Em seguida, esse resultado será comparado com o
%tempo de compilação utilizando a estrutura LTO, também utilizando as mesmas técnicas estatísticas.
%
%\subsection{Cronograma}
%
%As atividades serão dispostas pelos próximos meses na seguinte forma:
%
%\begin{figure}
%
%  \centering
%
%  \begin{ganttchart}{2019-05}{2020-3}
%    \gantttitlecalendar{year,month=shortname} \ganttnewline
%    \ganttgroup[progress=0]{Paralelização do GIMPLE}{2019-05}{2019-09} \ganttnewline
%    \ganttbar[progress=0]{Separar GIMPLE e RTL}{2019-05}{2019-05} \ganttnewline
%    \ganttbar[progress=0]{Remover estados Globais}{2019-05}{2019-07} \ganttnewline
%    \ganttbar[progress=0]{Paralelizar o GIMPLE}{2019-07}{2019-09} \ganttnewline
%    \ganttmilestone{Experimentos}{2019-09} \ganttnewline
%    \ganttgroup[progress=0]{Avaliar a paralelização do RTL e IPA}{2019-10}{2019-12} \ganttnewline
%    \ganttgroup[progress=0]{Escrita da Tese}{2019-11}{2020-02} \ganttnewline
%    \ganttbar[progress=0]{Escrita do Artigo}{2019-11}{2019-12} \ganttnewline
%    \ganttbar[progress=0]{Escrita do Tese}{2019-12}{2020-02} \ganttnewline
%    \ganttmilestone{Submissão}{2020-02}
%  \end{ganttchart}
%
%  \caption{Cronograma das Atividades.\label{fig:gantt}}
%\end{figure}
%
%\begin{itemize}
%	\item Separar GIMPLE e RTL: O RTL tem dependências com o \textit{Back End}
%	do GCC, sendo assim os passos GIMPLE e RTL serão separados para que seja
%	possível paralelizar os passos GIMPLE.
%
%	\item Remover estados globais: O GCC tem muitos estados globais referentes
%	aos passos GIMPLE que deverão ser adaptados para a paralelização.
%
%	\item Paralelizar o GIMPLE: Será adicionado código para que os passos do
%	GIMPLE executem em paralelo, e em seguida os experimentos serão executados.
%
%	\item Avaliar a paralelização do RTL e IPA: Dependendo do ganho obtido
%	na etapa anterior, será avaliado o ganho em paralelizar os passos IPA ou
%	o RTL.
%
%    \item Escrita de um Artigo Científico. Tempo destinado a escrita de um
%        artigo a respeito dos resultados do trabalho.
%
%	\item Escrita da Dissertação: Tempo destinado a escrita do texto final da
%        dissertação.
%\end{itemize}
%Uma ilustração da distribuição dessas tarefas no tempo está representada na
%Figura \ref{fig:gantt}.

\chapter{Experiments}\label{cap:experiments}

Having our methods been explained and implemented, we may now show our experiments'
methodology and results. We proceeded with the following methodology: we
analyzed which method gave us the best result using compilation time of
\texttt{gimple-match.c} as criteria, as well as correctness which we could
ensure in our implementation. Then we selected the best method for the
remaining three experiments:
\begin{itemize}
  \item Compiling every file individually using multiple numbers of threads;
  \item Compiling entire projects using multiple numbers of threads inside the
	compiler, together with \texttt{make -j} and comparing to \texttt{make -j} alone,
	which we define as our baseline. We then select one project (GCC itself) and
	show why we managed such speedup.
  \item Collecting the power draw for the CPU using multiple threads inside the
compiler and the baseline.
\end{itemize}

\section{Threaded Parallelism}

We start with our first approach, the threaded-based intraprocedural optimization
parallelism, as presented in Section \ref{sec:threads}.

\begin{subsection}{Methods}

We tested our implementation mainly using stress test with the gimple-match.c
file, which is the biggest file in the GCC project. We still have undetected
race conditions, which must be found and solved for a production-quality
implementation. Still, we have a $99.8\%$ of approval rate on gcc.dg testsuite (22
failures from 13183 test cases).

The computer used in this Benchmark had an Intel(R) Core(TM) i5-8250U CPU, with
8Gb of RAM.  Therefore, this computer had a CPU with 4 cores with
Hyperthreading, resulting in 8 virtual cores. All points are the mean of $n = 30$
samples, and the confidence interval to the populational mean was suppressed,
as the standard deviation was fairly low for a $95\%$ confidence interval. 

\end{subsection}

\begin{subsection}{Results}\label{sec:threaded-results}

Here we present our current performance results by parallelizing the GIMPLE
intraprocedural optimizations. It must be highlighted that this project still had
race conditions, and further, some locks can be
removed, as data structure can be duplicated and the garbage collector could be
deserialized.

Our parallelization technique resulted in a drop from 7 seconds to around 4
seconds with 2 threads, and around 3 seconds with 4 threads, resulting in a
speedup of $1.72\times$ and $2.52\times$, respectively. Here we can also see
that the use of Hyperthreading did not impact the result. This result was used to
estimate the improvement in RTL parallelization.

Next, Figure \ref{fig:parallel_real} shows this results when compared with the
total compilation time. Here we can see that there is a small improvement of
10\% when compiling this file.

However, since the same approach can be used to parallelize RTL, we can
estimate a speedup of $1.61\times$ in GCC when it gets parallelized by using
the speedup information obtained in GIMPLE, as illustrated in Figure
\ref{fig:parallel_estimate}.

\begin{figure}%[H]
 \centering
 \includegraphics[scale=0.7]{gimple_parallel.pdf}
 \caption{Results of parallelizing GIMPLE passes}
 \label{fig:parallel_gimple}
\end{figure}

\begin{figure}%[H]
 \centering
 \includegraphics[scale=0.7]{real.pdf}
 \caption{Speedup in total compilation time when parallelizing GIMPLE}
 \label{fig:parallel_real}
\end{figure}

\begin{figure}%[H]
 \centering
 \includegraphics[scale=0.7]{gcc_estimate.pdf}
 \caption{Estimate parallel time when also parallelizing RTL}
 \label{fig:parallel_estimate}
\end{figure}

\end{subsection}

\section{LTO-Based Parallelism}

In this section we present our experiments and results about our
LTO-Based parallelism technique, as presented \ref{sec:parallel_lto}.

\begin{subsection}{Methods}\label{sec:methods}

We ensure the correctness of our changes by:

\begin{enumerate}
\item Bootstrapping GCC.
\item Ensuring that the GCC testsuite was passing.
\item Compiling random programs generated with \textit{csmith} \citep{csmith}
and ensuring that the correct result was found.
\end{enumerate}

These tests were done with 2, 4, 8 and 64 parallel jobs, with minimum
partitioning quota of $10^3$ and $10^5$ instructions.

For the time measurements, all points represent the average of collected
samples, with errorbar representing a $95\%$ confidence interval.  Our test
files consisted of preprocessed source files from GCC, which compiles without
external includes. We collected $n = 15$ samples for every one of these files.
For the projects, we collected $n = 5$ samples because compilation is a
computer intensive task. We made sure that in every test, \texttt{-g0} was
passed for fairness, once in the current state we do not eliminate debug
symbols that are assigned to other partitions. If enabled, it imposes an
unfair overhead due to several unnecessary writes to disk because of duplicated
debug symbols, which slows down the parallel compilation process.

Tests were mainly executed in two computers, which are represented in Table
\ref{table:machines}. The graphic caption specifies where the test was run.
For the tests regarding the compilation of entire projects, we used the Opteron
machine with \texttt{make -j64} as the baseline; and then with \texttt{make
-j64} and enabling 8 internal threads in the compiler. We then enabled the
jobserver integration feature and let it use how many threads as GNU Make
allowed.

For the power usage measurement, we used the builtin AMD \texttt{fam15h\_power}
sensor available in the Opteron CPU \citep{fam15h}.

\begin{table}[]
\centering
\begin{tabular}{c|c|c|c|c|}
\cline{2-5}
                                      & Number of Cores & Number of Threads & RAM                                                    & Storage Device \\ \hline
\multicolumn{1}{|c|}{Core-i7 8650U}   & 4               & 8                 & \begin{tabular}[c]{@{}c@{}}8Gb\end{tabular} & SSD       \\ \hline
\multicolumn{1}{|c|}{4x Opteron 6376} & 32              & 64                & \begin{tabular}[c]{@{}c@{}}252Gb\end{tabular}   & HDD       \\ \hline
\end{tabular}
\caption{Machine specification of tests}
\label{table:machines}
\end{table}


The version of GCC used in the tests is available in the \texttt{autopar\_europar\_2021}
branch of \texttt{git://gcc.gnu.org/git/gcc.git}, with hash \texttt{e2da2f7205}.

\end{subsection}

\begin{subsection}{Results}\label{sec:lto-results}

We first highlight some of our results in Table \ref{table:files}. On this
table, the autogenerated column means that this file is compiled to C++, and
then compiled into assembly by GCC. Furthermore, Figure
\ref{fig:gcc_all_files} shows the timings and speedups of compiling files with
Number of Instructions (insns) > $5\times 10^4$. We used the insns as a metric
instead of Lines of Code (LoC) as it captures the template-heavy code better. It needs to
generate multiple functions for each argument from
the same input. In this figure, we can see that for large files, we mostly got
speedups in general. We managed speedups of up to a $2.4\times$ on Core-i7 when
compiling individual files with 8 threads, and up to $3.53\times$ on Opteron
6376 when with 64 threads. These speedup results are reasonably distant than
the linear optimal ($4.4$ if $p = 32$, from Equation \ref{eq:speedup}), so these results could be further improved.

We also notice that there is a file with $0.7\times$ speedup (\textit{i.e}., a
slowdown) named \texttt{brig-lang.c}. The reason for that is the existence of a
massive blob function that is generated when compiling this file (47261 insns,
while the file itself has 55927 insns).  This was not observed in the remaining
files. Since we partition the file by functions and global variables, and this
function consumes practically the entirety of compilation time, we were not
able to extract any meaningful parallelism here.


We will now discuss how our proposed changes impact the overall compilation
time of some projects. The objective is to check if this parallel file
compilation mechanism generalizes to other projects that do not have those
massive blob files, as in GCC.  For this, we run experiments compiling the
Linux Kernel 5.8-rc6, Git 2.30.0, the GCC version mentioned in Section
\ref{sec:methods} with and without bootstrap enabled, and JSON C++, with commit
hash \texttt{01f6b2e7418537}. We selected those projects as they are very often
used in the Open Source community.  We have only enabled jobserver integration
in GCC and Git, because it is necessary to modify an absolute large number of
Makefiles for that (for instance, Linux has 2597 Makefiles).

In Figure \ref{fig:gcc_projects} we show our results. We ran this benchmark on a
64-threaded Opteron machine instead of the Quad-Core i7 machine, once we find
reasonable that our methodology would have better results on high core count
machines, and close to none improvement on low-core count machines.  We can
observe a near $35\%$ improvement when compiling GCC with bootstrap disabled,
$25\%$ when bootstrap is enabled, and $15\%$ improvement when compiling Git
compared to \texttt{make -j64} alone. Our jobserver implementation also
squeezed a small improvement in GCC compilation, but showed a massive slowdown
in Git. This is because Jobserver integration has interprocess communication
cost with Make, which is a problem if the size of the partitions is small.
Other than this, we seen no significant speedup or slowdown in these
other projects.

We first analyzed why GCC had such a 35\% speedup. In Figure
\ref{fig:analysis_classical_parallel}, we plot the interval graph of each file
again, but in this time enabling threads inside the compiler. We can observe a
significant reduction in both file compilation time and in the parallelism bottleneck.
This is due to our parallel file compilation mechanism.

Now, this 15\% improvement on Git was not expected at first, which was an
excitement. When analyzing the compilation process of git, we found that it simply
did not have enough files to fully populate the 64-threaded Opteron machine.
Therefore, the extra parallelism added by our method improved the CPU usage
here.  However, this project compiles quite fast in this machine ($5s$), so the
improvement is not much noticeable in practice.

However, on the Linux kernel, we see no improvement nor slowdown by our
technique. The reason is that the files in this project compile very fast, and
it is evenly distributed through the processors' cores. The same thing applied
to the JSON C++ project.

Another interesting observation from both Figures \ref{fig:analysis_classical} and
\ref{fig:analysis_classical_parallel} is the high amount of serial steps when
compiling GCC until the high throughput compilation phase starts. This can be
attributed to GNU Autotools configuring the compiler for the environment of the
machine. Perhaps the configuration part could be parallelized as well, and this
could improve several projects across the board, or replace this tool
altogether with another that could better handle parallelism.

Furthermore, we also observe a reduction in power consumption when using this
mode, which is displayed in Figure \ref{fig:power}. This may be extra useful for Continuous Integration systems, which
may compile the same software from scratch several times a day.

\begin{table}[]
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|c|c|c|c|c|c|c|c}
\cline{2-7}
                                       & \multicolumn{3}{c|}{Core-i7}     & \multicolumn{3}{c|}{Opteron 6376}                                                                & \multicolumn{1}{l}{}               \\ \hline
\multicolumn{1}{|l|}{File}             & Sequential & 8 Threads & Speedup & \multicolumn{1}{l|}{Sequential} & \multicolumn{1}{l|}{64 Threads} & \multicolumn{1}{l|}{Speedup} & \multicolumn{1}{c|}{Autogenerated} & \multicolumn{1}{c|}{LoC}  \\ \hline
\multicolumn{1}{|l|}{gimple-match.c}   & $76s$        & $31s$       & $2.4\times$     & $221s$                            & $66s$                             & $3.32\times$                         & \multicolumn{1}{c|}{yes} & \multicolumn{1}{c|}{$244320$}          \\ \hline
\multicolumn{1}{|l|}{insn-emit.c}      & $23s$        & $10s$       & $2.25\times$    & $97s$                             & $37s$                             & $3.53\times$                         & \multicolumn{1}{c|}{yes}  & \multicolumn{1}{c|}{$273482$}         \\ \hline
\multicolumn{1}{|l|}{tree-vect-stmt.c} & $11s$        & $5s$        & $2.14\times$    & $32s$                             & $13s$                             & $2.46\times$                         & \multicolumn{1}{c|}{no}    & \multicolumn{1}{c|}{$12133$}        \\ \hline
\multicolumn{1}{|l|}{brig-lang.c} & $10s$        & $15	s$        & $0.7\times$    & $29s$                             & $35s$                             & $0.8\times$                         & \multicolumn{1}{c|}{no}    & \multicolumn{1}{c|}{$958$}        \\ \hline
\end{tabular}
}
\caption{Speedup of highlighted files}
\label{table:files}
\end{table}

\begin{figure}
\includegraphics[scale=0.7]{figuras/times-insns-crop.pdf}
\caption{Times of selected files with 1, 2, 4 and 8 threads on Core-i7}
\label{fig:gcc_all_files}
\end{figure}

\begin{figure}
\includegraphics[scale=0.7]{figuras/times-speedup-crop.pdf}
\caption{Speedups of selected files with 1, 2, 4 and 8 threads on Core-i7}
\label{fig:gcc_all_files}
\end{figure}

\begin{figure}
\centering
	 \includegraphics[scale=1]{figuras/experiment_projects_new-crop.pdf}
	  \caption{Compilation of some projects with 64 Makefile jobs and 8 threads in compiler in Opteron 6376}
	  \label{fig:gcc_projects}
\end{figure}

%\begin{landscape}
%%\begin{figure}[ht]
% \vspace*{-2cm}%
% \noindent%
% \hspace*{-2cm}%
%    \includegraphics[width=0.9\paperheight]{seq-crop.pdf}
%    \captionof{figure}{Compilation time of GCC without our modifications}
%    \label{fig:analysis_classical}
%%\end{figure}
%\end{landscape}

\begin{landscape}
%\begin{figure}[ht]
 \vspace*{-2cm}%
 \noindent%
 \hspace*{-2cm}%
    \includegraphics[width=0.9\paperheight]{par-crop.pdf}
    \captionof{figure}{Compilation time of GCC after our modifications}
    \label{fig:analysis_classical_parallel}
%\end{figure}
\end{landscape}

\begin{landscape}
%\begin{figure}[ht]
 \vspace*{-2cm}%
 \noindent%
 \hspace*{-2cm}%
    \includegraphics[width=0.9\paperheight]{power-crop.pdf}
    \captionof{figure}{Power usage comparison before and after our modifications}
    \label{fig:power}
%\end{figure}
\end{landscape}

\end{subsection}

%%%%%
