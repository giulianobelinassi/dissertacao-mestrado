\chapter{Conclusions}
\label{chap:conclusions}

Finally, in this chapter, we present the conclusions of our work. We did some
attempts on parallelizing an industrial-scale compiler, which some of them
worked very well for this situation, and others simply did not go so well.
Nevertheless, we managed to get a working implementation, with guaranteed
correct compilation and some remarkable results.

We approach this section by answering our Research Questions, presented in
Section \ref{sec:rq}, and we start by approaching \textbf{RQ1}. We show in
Section \ref{sec:profile} that the application of the intraprocedural
optimizations and code generation is a good candidate for parallelization since
in GCC it takes 75\% of compilation time.  For the parser, the related works
show that the parser can also be the target for parallelization, but our
experiments (in Figure \ref{fig:parallel_estimate}) show that the time required
for this step is minimal in GCC. The interprocedural optimizations, on the
other hand, are a challenge and if possible may also considerably speed up the
LTO in manycore machines. 

%We will now reaproach the Research Questions presented in Chapter
%\ref{cap:introducao}.  For \textbf{RQ1}, we show that parallelizing the
%intraprocedural optimizations can lead a speedup of up to $4\times$, and we
%show a reliable way of doing that on Subsection \ref{sec:parallel_lto}.  As for
%\textbf{RQ2}, we presented two ways of parallelizing a compiler, one archiving
%$1.68\times$ speedup and another with $2.4\times$ speedup on sample files using
%a quad-core processor.

Our first approach in parallelization involved the application of intraprocedural
optimizations in parallel to each function, using threads, as presented in
Section \ref{sec:threads}. Although this
predicted a $1.68\times$ speedup on GCC, its implementation showed to be
difficult because of the heavy use of global variables in the software.  We
even got to a point where analyzers such as helgrind and thread sanitizer were
not detecting the race conditions due to the race window being too small.
Therefore, this approach may not be adequate for already existing
industrial-scale compilers for adding parallelism.

However, if we are designing a compiler from the ground up, this approach may
be better from a software engineering perspective, as it becomes easier to add
multithreading to a part of the code that was, at least at some point, designed
with this in mind. We also get the advantage of avoiding the overheads
associated with process creation, such as page copying, etc.

Threading the compiler may also be the only option when parallelizing the
parser or interprocedural analysis, otherwise, there would be a huge amount of
interprocess communication. However, Figure \ref{fig:parallel_estimate} shows
that parsing is not a computer-intensive task. This is already parallel in LTO
by reading the multiple files, therefore this may not be of concern on present
days nor in the short future. However, the interprocess analysis is of concern,
once it is the only sequential step on LTO and may dominate the compilation
time on manycore machines just by being sequential.

On our second approach, we show how to use the already-existing LTO engine in
GCC to compile single files in parallel, as presented in Section
\ref{sec:parallel_lto}. The clear advantage of this approach is how fast we got
the first results when compared to threading the compiler, and how fast we got
it into a complete working condition (bootstrap the compiler, testsuite
acceptance, fuzzer testing).

Although the results were better than the threaded version ($1.88\times$ vs. $2.4\times$
on a Quad-Core),
one may argue that the reason for that is due to our threaded implementation
not being as optimized as the LTO based version -- which is true -- and the
contrary must be expected because the cost of launching a thread is smaller
than the cost of launching a process plus all the necessary page copying.
Therefore, we may expect even larger speedups when compared to the LTO version
on optimized implementations.

Therefore, as an answer for the \textbf{RQ2}, we show a method of archiving up
to $4\times$ speedup in theory, which when implemented yielded a speedup of up
to $3.52\times$. This method also yielded a speedup of up to $35\%$ when
compiling entire projects (in this case, GCC) when compared to \texttt{make
-j64} alone. The reason for this speedup is the existence of files with large
TU in the GCC project, as shown in Figure \ref{fig:analysis_classical}, and
this result might be interesting when the project uses autogenerated (blob) C++
files. We also found a small speedup on Git, which we find interesting because
it consists of small files. One explanation for this is that it does not have
enough files to fully populate the manycore machine's CPU.

From now on, we will focus on the LTO implementation results to drive our
conclusions. We archived up to $35\%$ speedup when comparing our results with
the \texttt{make -j64} alone, and no significant slowdown when compared to the
sequential version (even if there was, the developer could simply disable the
parallel compilation if this is observed). Furthermore, we show that
partitioning files with TU larger than $10^3$ yields speedups, which answer
\textbf{RQ4}. The reason for this is the existence of files with large TU in
the GCC project, as shown in Figure \ref{fig:analysis_classical}, and this
result might be interesting when the project uses autogenerated (blob) C++
files. We also found a small speedup on Git, which we find interesting because
it consists of small files. One explanation for this is that it does not have
enough files to fully populate the manycore machine's CPU.

As for the electrical energy usage, we show a reduction in power draw by 72\% the
CPU when compiling GCC with our options enabled, which answers \textbf{RQ5}. On
Figure \ref{fig:power}, we observe a significant reduction in the peak power
usage on the parallel version. It may be that there is a point when the
processor is in partial usage (not in full usage) and may draw the same amount
of power as if it is in full usage. Therefore, for better energy efficiency, we
should use the maximum power of the CPU in the shortest amount of time to get a
task done -- and this will also save energy --.

For future works, we concentrate on the following topics:

\begin{enumerate}

\item Fix the already-know bugs on the LTO implementation. There is (1) a bug related
to how our partitioner applier handle nodes created by the \textit{ipa-split}
pass, and therefore we have disabled it for now; and (2) our partitioner
applier do not remove debug symbols associated with removed nodes, resulting in
unknown symbols being dumped into the final assembly. These bugs certainly prevents the
current branch from being used in industrial environments (which is a good
reason why this was not merged in upstream yet), but they are fine as a proof
of concept to support our claims.

\item Implementing a better partitioner to the LTO project. One main issue
with our partitioner is that we kept its load balancing algorithm minimal to
ensure that it works. Using the LTO default partitioner as a base is a good start.

\item Modify the driver to also support external compiler through GCC
SPEC language. Our current implementation only checks if launching program
is a known compiler/assembler/linker, and will get confused in languages that
needs additional steps (such as CUDA).

\item Try to develop a predictive model to decide if the input file is
a good candidate for parallel compilation. Fig. \ref{fig:gcc_all_files} shows
a clear linear correlation between the expected number of instructions and time
(and maybe it is the best parameter), but it may be possible to (statically)
collect more information about the file for a better decision.

\item Try to avoid partial linking and symbol promotion altogether by
concatenating the generated assembly files, instead of linking every generated
assembly file into temporary object files. This is extra useful for
architectures that require extra steps to generate code that must be globally
available across the program, which may generate a slower code when compared to
the non-public promoted version.

\item Research about how Interprocedural Optimizations can be implemented
in parallel in GCC. This not only improves our parallel implementation,
but also might improve the parallelism in LTO mode.

\end{enumerate}

Finally, we expect that our work is relevant for the near future of computing.
As illustrated by \cite{42years}, the CPU manufacturers are expanding on the
number of cores rather than raw sequential speed in recent days. We are already
seeing manufacturers build 64-cores, 128-threads CPUs for workstations, and we
may see CPUs with even more cores in the future unless some new technology
comes up that revolutionizes how CPUs are manufactured and we can see the
sequential performance increase of early 2000 years again.

%%%%%
