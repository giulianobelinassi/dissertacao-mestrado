\chapter{Conclusions}
\label{chap:conclusions}

We have presented two ways of parallelizing a compiler. Our first approach
involved parallelizing the application of intraprocedural optimizations in
parallel to each function, using threads. Although this predicted a $1.68\times$
speedup on GCC, its implementation showed to be difficult because of the heavy
use of global variables in the software.  We even got to a point where
analyzers such as helgrind and thread sanitizer were not detecting the race
conditions due to the race window being too small. Therefore, this approach may
not be adequate for already existing industrial-scale compilers for adding
parallelism.

However, if we are designing a compiler from the ground up, this approach may
be better from a software engineering perspective, as it becomes easier to add
multithreading to a part of the code that was, at least at some point, designed
with this in mind. We also get the advantage of avoiding the overheads
associated with process creation, such as page copying, etc.

Threading the compiler may also be the only option when parallelizing the
parser or interprocedural analysis, otherwise, there would be a huge amount of
interprocess communication. However, Figure \ref{fig:parallel_estimate} shows
that parsing is not a computer-intensive task. This is already parallel in LTO
by reading the multiple files, therefore this may not be of concern on present
days nor in the short future. However, the interprocess analysis is of concern,
once it is the only sequential step on LTO and may dominate the compilation
time on manycore machines just by being sequential.

On our second approach, we show how to use the already-existing LTO engine in
GCC to compile single files in parallel. The clear advantage of this approach
is how fast we got the first results when compared to threading the compiler,
and how fast we got it into a complete working condition (bootstrap the
compiler, testsuite acceptance, fuzzer testing).

Although the results were better than the threaded version ($1.88\times$ vs. $2.4\times$),
one may argue that the reason for that is due to our threaded implementation
not being as optimized as the LTO based version -- which is true -- and the
contrary must be expected because the cost of launching a thread is smaller
than the cost of launching a process plus all the necessary page copying.
Therefore, we may expect even larger speedups when compared to the LTO version
on optimized implementations.

From now on, we will focus on the LTO implementation results to drive our
conclusions. We archived up to $35\%$ speedup when comparing our results with
the \texttt{make -j64} alone, and no significant slowdown when compared to the
sequential version (even if there was, the developer could simply disable the
parallel compilation if this is observed). The reason for this is the existence
of files with large TU in the GCC project, as shown in Figure
\ref{fig:analysis_classical}, and this result might be interesting when the project
uses autogenerated (blob) C++ files. We also found a small speedup on Git,
which we find interesting because it consists of small files. One explanation
for this is that it does not have enough files to fully populate the manycore
machine's CPU.

As for the electrical energy usage, we show a reduction in power draw by the
CPU when compiling GCC with our options enabled. On Figure \ref{fig:power},
we observe a significant reduction in the peak power usage on the parallel
version. It may be that there is a point when the processor is in partial usage
(not in full usage) and may draw the same amount of power as if it is in full
usage. Therefore, for better energy efficiency, we should use the maximum power
of the CPU in the shortest amount of time to get a task done -- and this will
also save energy --.
%%%%%
